#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå‰§æœ¬åŠ è½½å™¨
æ”¯æŒmetaè¯æ¡æ£€éªŒã€dataclassæ ¼å¼æ ¡éªŒã€éŸ³ç´ æ£€éªŒã€ç¼“å­˜ç­‰åŠŸèƒ½
"""

import json
import logging
import hashlib
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import asdict, is_dataclass
from datetime import datetime

from app.models.models import Meta, Style, Cue, SubtitleDocument
from app.core.g2p.base import G2PConverter
try:
    from app.utils.script_conversion_utils import ScriptConverter
    CONVERSION_AVAILABLE = True
except ImportError:
    ScriptConverter = None
    CONVERSION_AVAILABLE = False


class ScriptValidationError(Exception):
    """å‰§æœ¬éªŒè¯é”™è¯¯"""
    pass


class EnhancedScriptLoader:
    """å¢å¼ºç‰ˆå‰§æœ¬åŠ è½½å™¨"""
    
    def __init__(self, g2p_converter: Optional[G2PConverter] = None, head_tail_count: int = 5):
        self.g2p_converter = g2p_converter
        self.validation_results = {}
        self.conversion_results = {}
        self.head_tail_count = head_tail_count  # å¤´éƒ¨å’Œå°¾éƒ¨è¯è¯­æ•°é‡
        self.cache_dir = Path("cache/scripts")  # ç¼“å­˜ç›®å½•
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def load_script(self, filepath: str) -> Tuple[SubtitleDocument, Dict[str, Any]]:
        """
        åŠ è½½å‰§æœ¬æ–‡ä»¶
        
        Returns:
            Tuple[SubtitleDocument, Dict]: (åŠ è½½çš„æ–‡æ¡£, åŠ è½½æŠ¥å‘Š)
        """
        file_path = Path(filepath)
        if not file_path.exists():
            raise FileNotFoundError(f"å‰§æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
        print(f"ğŸ” å¼€å§‹åŠ è½½å‰§æœ¬: {file_path.name}")
        
        # 1. è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        file_hash = self._calculate_file_hash(file_path)
        print(f"ğŸ” æ–‡ä»¶å“ˆå¸Œ: {file_hash[:16]}...")
        
        # 2. æ£€æŸ¥ç¼“å­˜
        cached_document = self._load_from_cache(file_hash)
        if cached_document:
            print("âœ… ä»ç¼“å­˜åŠ è½½æˆåŠŸ")
            report = self._generate_cache_report(cached_document)
            return cached_document, report
        
        # 3. åŠ è½½JSONæ•°æ®
        raw_data = self._load_json(file_path)
        
        # 4. æ£€æŸ¥æ˜¯å¦æœ‰metaè¯æ¡
        has_meta = self._check_meta_field(raw_data)
        
        if not has_meta:
            print("âš ï¸ æœªå‘ç°metaè¯æ¡ï¼Œè°ƒç”¨è½¬æ¢è„šæœ¬...")
            raw_data = self._convert_legacy_format(raw_data, file_path)
            
        # 5. è¿›è¡Œdataclassæ ¼å¼æ ¡éªŒ
        print("ğŸ” è¿›è¡Œdataclassæ ¼å¼æ ¡éªŒ...")
        document = self._validate_and_create_document(raw_data)
        
        # 6. è®¾ç½®æ–‡ä»¶å“ˆå¸Œåˆ°metaä¸­
        document.meta.hash = file_hash
        document.meta.updated_at = datetime.now().isoformat()
        
        # 7. æ£€æŸ¥éŸ³ç´ å¹¶è¿›è¡ŒG2På¤„ç†
        print("ğŸ” æ£€æŸ¥éŸ³ç´ æ•°æ®...")
        g2p_results = self._process_phonemes(document)
        
        # 8. å¤„ç†å¤´éƒ¨å’Œå°¾éƒ¨è¯è¯­
        print("ğŸ” å¤„ç†å¤´éƒ¨å’Œå°¾éƒ¨è¯è¯­...")
        head_tail_results = self._process_head_tail_tokens(document)
        
        # 9. å¤„ç†æ•´å¥n-gramç”Ÿæˆ
        print("ğŸ” ç”Ÿæˆæ•´å¥n-gramç‰¹å¾...")
        ngram_results = self._process_line_ngrams(document, n=2)
        
        # 10. ä¿å­˜åˆ°ç¼“å­˜
        self._save_to_cache(document, file_hash)
        
        # 11. ç”ŸæˆåŠ è½½æŠ¥å‘Š
        report = self._generate_load_report(document, g2p_results, head_tail_results, ngram_results)
        
        print("âœ… å‰§æœ¬åŠ è½½å®Œæˆ")
        return document, report
        
    def _load_json(self, filepath: Path) -> Dict[str, Any]:
        """åŠ è½½JSONæ–‡ä»¶"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… JSONæ–‡ä»¶åŠ è½½æˆåŠŸ: {len(str(data))} å­—ç¬¦")
            return data
        except json.JSONDecodeError as e:
            raise ScriptValidationError(f"JSONæ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            raise ScriptValidationError(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            
    def _check_meta_field(self, data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰metaè¯æ¡"""
        has_meta = 'meta' in data
        if has_meta:
            print("âœ… å‘ç°metaè¯æ¡ï¼Œä½¿ç”¨æ–°æ ¼å¼")
        else:
            print("âš ï¸ æœªå‘ç°metaè¯æ¡ï¼Œæ£€æµ‹åˆ°æ—§æ ¼å¼")
        return has_meta
        
    def _convert_legacy_format(self, data: Dict[str, Any], filepath: Path) -> Dict[str, Any]:
        """è½¬æ¢æ—§æ ¼å¼åˆ°æ–°æ ¼å¼"""
        print("ğŸ”„ è½¬æ¢æ—§æ ¼å¼åˆ°æ–°æ ¼å¼...")
        
        # åˆ›å»ºé»˜è®¤metaä¿¡æ¯
        meta = {
            "title": filepath.stem,
            "author": "",
            "translator": "",
            "version": "1.0",
            "description": f"ä»æ—§æ ¼å¼è½¬æ¢: {filepath.name}",
            "language": [],
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "license": ""
        }
        
        # åˆ›å»ºæ–°æ ¼å¼æ•°æ®
        new_data = {
            "meta": meta,
            "styles": {
                "default": {
                    "font": "Noto Sans",
                    "size": 42,
                    "color": "#FFFFFF",
                    "pos": "bottom"
                }
            },
            "cues": data.get("cues", [])
        }
        
        # æ£€æµ‹å¹¶æ·»åŠ è¯­è¨€ä¿¡æ¯
        languages = self._detect_languages(new_data["cues"])
        new_data["meta"]["language"] = languages
        
        print(f"âœ… æ ¼å¼è½¬æ¢å®Œæˆï¼Œæ£€æµ‹åˆ°è¯­è¨€: {languages}")
        return new_data
    
    def load_converted_script(self, filepath: str, validate_ngrams: bool = True) -> Tuple[SubtitleDocument, Dict[str, Any]]:
        """
        åŠ è½½å·²è½¬æ¢çš„å‰§æœ¬æ–‡ä»¶ï¼ˆåŒ…å«n-gramç‰¹å¾ï¼‰
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            validate_ngrams: æ˜¯å¦éªŒè¯n-gramç‰¹å¾
            
        Returns:
            Tuple[SubtitleDocument, Dict]: (åŠ è½½çš„æ–‡æ¡£, åŠ è½½æŠ¥å‘Š)
        """
        file_path = Path(filepath)
        if not file_path.exists():
            raise FileNotFoundError(f"å‰§æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
        print(f"ğŸ” å¼€å§‹åŠ è½½è½¬æ¢æ ¼å¼å‰§æœ¬: {file_path.name}")
        
        # åŠ è½½JSONæ•°æ®
        data = self._load_json(file_path)
        
        # åˆå§‹åŒ–æŠ¥å‘Š
        report = {
            "file_path": str(file_path),
            "format_type": "converted",
            "has_ngrams": False,
            "ngram_stats": {},
            "validation_results": {},
            "loading_time": "",
            "total_cues": 0,
            "characters": [],
            "languages": []
        }
        
        start_time = datetime.now()
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è½¬æ¢æ ¼å¼
            if 'cues' not in data:
                raise ScriptValidationError("è½¬æ¢æ ¼å¼æ–‡ä»¶å¿…é¡»åŒ…å«'cues'å­—æ®µ")
            
            cues_data = data['cues']
            report["total_cues"] = len(cues_data)
            
            # æ£€æŸ¥n-gramç‰¹å¾
            ngram_features = self._analyze_ngram_features(cues_data)
            report["has_ngrams"] = ngram_features["has_ngrams"]
            report["ngram_stats"] = ngram_features
            
            # è½¬æ¢ä¸ºCueå¯¹è±¡
            cues = []
            characters = set()
            languages = set()
            
            for cue_data in cues_data:
                # åˆ›å»ºCueå¯¹è±¡ï¼Œæ”¯æŒæ–°çš„n-gramå­—æ®µ
                cue = Cue(
                    id=cue_data.get('id', 0),
                    character=cue_data.get('character'),
                    line=cue_data.get('line', ''),
                    pure_line=cue_data.get('pure_line', ''),
                    phonemes=cue_data.get('phonemes', ''),
                    character_cue_index=cue_data.get('character_cue_index', -1),
                    translation=cue_data.get('translation', {}),
                    notes=cue_data.get('notes', ''),
                    style=cue_data.get('style', 'default'),
                    head_tok=cue_data.get('head_tok', []),
                    head_phonemes=cue_data.get('head_phonemes', []),
                    tail_tok=cue_data.get('tail_tok', []),
                    tail_phonemes=cue_data.get('tail_phonemes', []),
                    line_ngram=self._convert_to_tuples(cue_data.get('line_ngram', [])),
                    line_ngram_phonemes=self._convert_to_tuples(cue_data.get('line_ngram_phonemes', []))
                )
                
                cues.append(cue)
                
                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                if cue.character:
                    characters.add(cue.character)
                
                if cue.translation:
                    languages.update(cue.translation.keys())
            
            # åˆ›å»ºé»˜è®¤metaå’Œstylesï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            meta = Meta(
                title=file_path.stem,
                author="",
                language=list(languages),
                created_at=datetime.now().isoformat()
            )
            
            styles = {"default": Style()}
            
            # åˆ›å»ºæ–‡æ¡£
            document = SubtitleDocument(
                meta=meta,
                styles=styles,
                cues=cues
            )
            
            # å®ŒæˆæŠ¥å‘Š
            report["characters"] = sorted(list(characters))
            report["languages"] = sorted(list(languages))
            report["loading_time"] = str(datetime.now() - start_time)
            
            # éªŒè¯n-gramsï¼ˆå¦‚æœéœ€è¦ï¼‰
            if validate_ngrams and report["has_ngrams"]:
                validation_results = self._validate_ngrams(cues)
                report["validation_results"]["ngrams"] = validation_results
            
            print(f"âœ… è½¬æ¢æ ¼å¼å‰§æœ¬åŠ è½½æˆåŠŸ")
            return document, report
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            raise ScriptValidationError(f"è½¬æ¢æ ¼å¼å‰§æœ¬åŠ è½½å¤±è´¥: {e}")

    def _analyze_ngram_features(self, cues_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æn-gramç‰¹å¾"""
        ngram_fields = ['line_ngram', 'line_ngram_phonemes']
        
        stats = {
            "has_ngrams": False,
            "total_cues_with_ngrams": 0,
            "ngram_field_counts": {},
            "average_ngrams_per_cue": 0,
            "ngram_size_distribution": {}
        }
        
        total_ngrams = 0
        cues_with_ngrams = 0
        size_counts = {}
        
        for field in ngram_fields:
            stats["ngram_field_counts"][field] = 0
        
        for cue_data in cues_data:
            cue_has_ngrams = False
            cue_ngram_count = 0
            
            for field in ngram_fields:
                if field in cue_data and cue_data[field]:
                    stats["ngram_field_counts"][field] += 1
                    cue_has_ngrams = True
                    
                    # ç»Ÿè®¡n-gramæ•°é‡å’Œå¤§å°
                    for ngram in cue_data[field]:
                        if isinstance(ngram, (list, tuple)):
                            total_ngrams += 1
                            cue_ngram_count += 1
                            size = len(ngram)
                            size_counts[size] = size_counts.get(size, 0) + 1
            
            if cue_has_ngrams:
                cues_with_ngrams += 1
        
        stats["has_ngrams"] = cues_with_ngrams > 0
        stats["total_cues_with_ngrams"] = cues_with_ngrams
        stats["average_ngrams_per_cue"] = total_ngrams / max(len(cues_data), 1)
        stats["ngram_size_distribution"] = size_counts
        
        return stats

    def _convert_to_tuples(self, ngram_list: List) -> List[tuple]:
        """
        å°†n-gramåˆ—è¡¨è½¬æ¢ä¸ºtupleåˆ—è¡¨
        å¤„ç†JSONååºåˆ—åŒ–æ—¶listè½¬tupleçš„é—®é¢˜
        
        Args:
            ngram_list: n-gramåˆ—è¡¨ï¼Œå¯èƒ½åŒ…å«listæˆ–å·²ç»æ˜¯tuple
            
        Returns:
            List[tuple]: è½¬æ¢åçš„tupleåˆ—è¡¨
        """
        if not ngram_list:
            return []
        
        result = []
        for item in ngram_list:
            if isinstance(item, (list, tuple)):
                result.append(tuple(item))
            else:
                # å•ä¸ªå…ƒç´ ï¼ŒåŒ…è£…æˆtuple
                result.append((item,))
        
        return result

    def _validate_ngrams(self, cues: List[Cue]) -> Dict[str, Any]:
        """éªŒè¯n-gramç‰¹å¾çš„ä¸€è‡´æ€§"""
        validation = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "total_checked": len(cues)
        }
        
        for i, cue in enumerate(cues):
            # æ£€æŸ¥line_ngramçš„åŸºæœ¬å®Œæ•´æ€§
            if cue.line_ngram:
                for ngram in cue.line_ngram:
                    if isinstance(ngram, (list, tuple)):
                        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šéªŒè¯é€»è¾‘
                        if len(ngram) == 0:
                            validation["warnings"].append(
                                f"Cue {cue.id}: å‘ç°ç©ºçš„line_ngram"
                            )
            
            # æ£€æŸ¥éŸ³ç´ n-gramçš„å®Œæ•´æ€§
            if cue.line_ngram_phonemes:
                for ngram in cue.line_ngram_phonemes:
                    if isinstance(ngram, (list, tuple)):
                        if len(ngram) == 0:
                            validation["warnings"].append(
                                f"Cue {cue.id}: å‘ç°ç©ºçš„line_ngram_phonemes"
                            )
        
        return validation

    def convert_and_load(self, input_file: str, language: str = 'fra-Latn', n: int = 2) -> Tuple[SubtitleDocument, Dict[str, Any]]:
        """
        è½¬æ¢å¹¶åŠ è½½å‰§æœ¬æ–‡ä»¶
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            language: è¯­è¨€ä»£ç 
            n: n-gramå¤§å°
            
        Returns:
            Tuple[SubtitleDocument, Dict]: (åŠ è½½çš„æ–‡æ¡£, åŠ è½½æŠ¥å‘Š)
        """
        print(f"ğŸ”„ å¼€å§‹è½¬æ¢å’ŒåŠ è½½å‰§æœ¬: {input_file}")
        
        # ç”Ÿæˆä¸´æ—¶è¾“å‡ºæ–‡ä»¶å
        input_path = Path(input_file)
        temp_output = input_path.parent / f"{input_path.stem}_converted_temp.json"
        
        try:
            # æ‰§è¡Œè½¬æ¢
            if not CONVERSION_AVAILABLE or not ScriptConverter:
                raise ScriptValidationError("è½¬æ¢å·¥å…·ä¸å¯ç”¨")
                
            converter = ScriptConverter(language=language, use_fallback=True)
            success = converter.convert_script(
                input_file=str(input_path),
                output_file=str(temp_output),
                n=n,
                verbose=False  # å‡å°‘è¾“å‡º
            )
            
            if not success:
                raise ScriptValidationError("å‰§æœ¬è½¬æ¢å¤±è´¥")
            
            # åŠ è½½è½¬æ¢åçš„æ–‡ä»¶
            document, report = self.load_converted_script(str(temp_output))
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_output.exists():
                temp_output.unlink()
            
            # ç»Ÿä¸€æŠ¥å‘Šæ ¼å¼ï¼Œæ·»åŠ ç¼ºå¤±çš„file_infoå­—æ®µ
            if "file_info" not in report:
                report["file_info"] = {
                    "title": document.meta.title,
                    "author": document.meta.author,
                    "version": document.meta.version,
                    "languages": document.meta.language,
                    "total_cues": len(document.cues),
                    "hash": getattr(document.meta, 'hash', None)
                }
            
            report["conversion_performed"] = True
            report["conversion_params"] = {"language": language, "n": n}
            
            return document, report
            
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_output.exists():
                temp_output.unlink()
            raise ScriptValidationError(f"è½¬æ¢å’ŒåŠ è½½å¤±è´¥: {e}")

        return new_data
        
    def _detect_languages(self, cues: List[Dict]) -> List[str]:
        """æ£€æµ‹å‰§æœ¬ä¸­çš„è¯­è¨€"""
        languages = set()
        
        for cue in cues:
            # æ£€æŸ¥translationå­—æ®µ
            if "translation" in cue and isinstance(cue["translation"], dict):
                languages.update(cue["translation"].keys())
                
        # æ·»åŠ åŸå§‹è¯­è¨€ï¼ˆå‡è®¾ä¸ºæ³•è¯­ï¼Œæ ¹æ®æ‚¨çš„å‰§æœ¬å†…å®¹ï¼‰
        if any("character" in cue and cue.get("line", "") for cue in cues):
            languages.add("fr")  # æ³•è¯­
            
        return sorted(list(languages))
        
    def _validate_and_create_document(self, data: Dict[str, Any]) -> SubtitleDocument:
        """éªŒè¯æ•°æ®æ ¼å¼å¹¶åˆ›å»ºæ–‡æ¡£å¯¹è±¡"""
        errors = []
        
        try:
            # éªŒè¯metaå­—æ®µ
            meta_data = data.get("meta", {})
            meta = Meta(**meta_data) if meta_data else Meta()
            
            # éªŒè¯styleså­—æ®µ
            styles_data = data.get("styles", {"default": {}})
            styles = {}
            for name, style_data in styles_data.items():
                try:
                    styles[name] = Style(**style_data)
                except Exception as e:
                    errors.append(f"æ ·å¼ '{name}' æ ¼å¼é”™è¯¯: {e}")
                    styles[name] = Style()  # ä½¿ç”¨é»˜è®¤æ ·å¼
                    
            # éªŒè¯cueså­—æ®µ
            cues_data = data.get("cues", [])
            cues = []
            
            for i, cue_data in enumerate(cues_data):
                try:
                    # å¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
                    cue_dict = self._normalize_cue_data(cue_data, i)
                    cue = Cue(**cue_dict)
                    cues.append(cue)
                except Exception as e:
                    errors.append(f"å°è¯ {i+1} æ ¼å¼é”™è¯¯: {e}")
                    # åˆ›å»ºä¸€ä¸ªæœ€å°çš„æœ‰æ•ˆcue
                    cues.append(Cue(
                        id=i+1,
                        character=cue_data.get("character"),
                        line=cue_data.get("line", ""),
                        phonemes="",
                        notes=f"æ ¼å¼é”™è¯¯: {e}"
                    ))
                    
            if errors:
                print(f"âš ï¸ å‘ç° {len(errors)} ä¸ªæ ¼å¼é—®é¢˜ï¼š")
                for error in errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    print(f"   - {error}")
                if len(errors) > 5:
                    print(f"   ... è¿˜æœ‰ {len(errors)-5} ä¸ªé”™è¯¯")
                    
            document = SubtitleDocument(meta=meta, styles=styles, cues=cues)
            print(f"âœ… æ–‡æ¡£åˆ›å»ºæˆåŠŸ: {len(cues)} æ¡å°è¯")
            
            self.validation_results = {
                "total_cues": len(cues),
                "valid_cues": len([c for c in cues if c.line]),
                "errors": errors,
                "has_meta": True,
                "languages": document.get_all_languages()
            }
            
            return document
            
        except Exception as e:
            raise ScriptValidationError(f"æ–‡æ¡£åˆ›å»ºå¤±è´¥: {e}")
            
    def _normalize_cue_data(self, cue_data: Dict[str, Any], index: int) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–cueæ•°æ®ï¼Œè¡¥å……ç¼ºå¤±å­—æ®µ"""
        # ç”Ÿæˆpure_lineï¼ˆå¦‚æœåŸæ•°æ®ä¸­æ²¡æœ‰ï¼‰
        line = cue_data.get("line", "")
        pure_line = cue_data.get("pure_line", "")
        if not pure_line and line:
            # å¦‚æœæ²¡æœ‰pure_lineï¼Œä»lineç”Ÿæˆä¸€ä¸ªæ¸…ç†ç‰ˆæœ¬
            pure_line = self._clean_text_for_ngram(line)
        
        normalized = {
            "id": cue_data.get("id", index + 1),
            "character": cue_data.get("character"),
            "line": line,
            "pure_line": pure_line,
            "phonemes": cue_data.get("phonemes", ""),
            "character_cue_index": cue_data.get("character_cue_index", -1),
            "translation": cue_data.get("translation", {}),
            "notes": cue_data.get("notes", ""),
            "style": cue_data.get("style", "default"),
            # æ–°å¢å¤´å°¾å­—æ®µ
            "head_tok": cue_data.get("head_tok", []),
            "head_phonemes": cue_data.get("head_phonemes", []),
            "tail_tok": cue_data.get("tail_tok", []),
            "tail_phonemes": cue_data.get("tail_phonemes", []),
            # æ–°å¢æ•´å¥n-gramå­—æ®µï¼ˆè¿›è¡Œç±»å‹è½¬æ¢ï¼‰
            "line_ngram": self._convert_to_tuples(cue_data.get("line_ngram", [])),
            "line_ngram_phonemes": self._convert_to_tuples(cue_data.get("line_ngram_phonemes", []))
        }
        
        # ç¡®ä¿translationæ˜¯å­—å…¸
        if not isinstance(normalized["translation"], dict):
            normalized["translation"] = {}
        
        # ç¡®ä¿å¤´å°¾å­—æ®µæ˜¯åˆ—è¡¨
        for field in ["head_tok", "head_phonemes", "tail_tok", "tail_phonemes"]:
            if not isinstance(normalized[field], list):
                normalized[field] = []
        
        # ngramå­—æ®µå·²ç»åœ¨ä¸Šé¢å¤„ç†äº†ç±»å‹è½¬æ¢
            
        return normalized
        
    def _process_phonemes(self, document: SubtitleDocument) -> Dict[str, Any]:
        """å¤„ç†éŸ³ç´ æ•°æ®"""
        if not self.g2p_converter:
            print("âš ï¸ æœªæä¾›G2Pè½¬æ¢å™¨ï¼Œè·³è¿‡éŸ³ç´ å¤„ç†")
            return {"processed": 0, "skipped": len(document.cues), "errors": []}
            
        processed = 0
        skipped = 0
        errors = []
        
        # æ”¶é›†éœ€è¦G2På¤„ç†çš„å°è¯
        lines_to_process = []
        indices_to_process = []
        
        for i, cue in enumerate(document.cues):
            if not cue.line.strip():  # ç©ºå°è¯
                skipped += 1
                continue
                
            if not cue.phonemes or not cue.phonemes.strip():  # æ²¡æœ‰éŸ³ç´ 
                # ä¼˜å…ˆä½¿ç”¨pure_lineï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ¸…ç†åçš„line
                text_for_g2p = cue.pure_line if cue.pure_line else self._clean_text_for_ngram(cue.line)
                if text_for_g2p.strip():
                    lines_to_process.append(text_for_g2p)
                    indices_to_process.append(i)
                else:
                    skipped += 1
            else:
                print(f"   è·³è¿‡å·²æœ‰éŸ³ç´ çš„å°è¯ {cue.id}: '{cue.line[:30]}...'")
                skipped += 1
                
        if lines_to_process:
            print(f"ğŸ”„ å¯¹ {len(lines_to_process)} æ¡å°è¯è¿›è¡ŒG2Pè½¬æ¢...")
            
            try:
                # æ‰¹é‡G2Pè½¬æ¢
                phonemes_results = self.g2p_converter.batch_convert(lines_to_process)
                
                # æ›´æ–°cueå¯¹è±¡çš„éŸ³ç´ 
                for idx, phonemes in zip(indices_to_process, phonemes_results):
                    document.cues[idx].phonemes = phonemes
                    processed += 1
                    if processed <= 3:  # æ˜¾ç¤ºå‰3ä¸ªè½¬æ¢ç»“æœ
                        cue = document.cues[idx]
                        print(f"   âœ… {cue.id}: '{cue.line[:20]}...' -> '{phonemes[:30]}...'")
                        
                if processed > 3:
                    print(f"   ... è¿˜æœ‰ {processed-3} æ¡å°è¯å®Œæˆè½¬æ¢")
                    
            except Exception as e:
                error_msg = f"G2Pæ‰¹é‡è½¬æ¢å¤±è´¥: {e}"
                errors.append(error_msg)
                print(f"âŒ {error_msg}")
        else:
            print("âœ… æ‰€æœ‰å°è¯éƒ½å·²æœ‰éŸ³ç´ æ•°æ®")
            
        return {
            "processed": processed,
            "skipped": skipped,
            "errors": errors,
            "total": len(document.cues)
        }
    
    def _calculate_file_hash(self, filepath: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hasher = hashlib.md5()
        with open(filepath, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def _load_from_cache(self, file_hash: str) -> Optional[SubtitleDocument]:
        """ä»ç¼“å­˜åŠ è½½æ–‡æ¡£"""
        cache_file = self.cache_dir / f"{file_hash}.cache"
        if not cache_file.exists():
            return None
            
        try:
            with open(cache_file, 'rb') as f:
                cached_data = pickle.load(f)
                
            # éªŒè¯ç¼“å­˜ç‰ˆæœ¬å…¼å®¹æ€§
            if cached_data.get('version') != '1.0':
                print("âš ï¸ ç¼“å­˜ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œé‡æ–°åŠ è½½")
                return None
                
            document = cached_data.get('document')
            if isinstance(document, SubtitleDocument):
                return document
                
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            
        return None
    
    def _save_to_cache(self, document: SubtitleDocument, file_hash: str):
        """ä¿å­˜æ–‡æ¡£åˆ°ç¼“å­˜"""
        try:
            cache_file = self.cache_dir / f"{file_hash}.cache"
            cache_data = {
                'version': '1.0',
                'document': document,
                'cached_at': datetime.now().isoformat()
            }
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
                
            print(f"âœ… å·²ä¿å­˜åˆ°ç¼“å­˜: {cache_file.name}")
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def _generate_cache_report(self, document: SubtitleDocument) -> Dict[str, Any]:
        """ç”Ÿæˆç¼“å­˜åŠ è½½æŠ¥å‘Š"""
        return {
            "file_info": {
                "title": document.meta.title,
                "author": document.meta.author,
                "version": document.meta.version,
                "languages": document.meta.language,
                "total_cues": len(document.cues),
                "hash": document.meta.hash
            },
            "validation": {"from_cache": True},
            "g2p_processing": {"from_cache": True},
            "head_tail_processing": {"from_cache": True},
            "summary": {
                "valid_cues": len([c for c in document.cues if c.line.strip()]),
                "empty_cues": len([c for c in document.cues if not c.line.strip()]),
                "cues_with_phonemes": len([c for c in document.cues if c.phonemes and c.phonemes.strip()]),
                "cues_with_head_tail": len([c for c in document.cues if c.head_tok or c.tail_tok]),
                "cues_with_line_ngrams": len([c for c in document.cues if c.line_ngram]),
                "characters": len(set(c.character for c in document.cues if c.character)),
                "available_languages": getattr(document, 'get_all_languages', lambda: [])()
            }
        }
    
    def _process_head_tail_tokens(self, document: SubtitleDocument) -> Dict[str, Any]:
        """å¤„ç†å¤´éƒ¨å’Œå°¾éƒ¨è¯è¯­æå–å’ŒG2Pè½¬æ¢"""
        if not self.g2p_converter:
            print("âš ï¸ æœªæä¾›G2Pè½¬æ¢å™¨ï¼Œè·³è¿‡å¤´å°¾è¯è¯­å¤„ç†")
            return {"processed": 0, "skipped": len(document.cues), "errors": []}
        
        processed = 0
        skipped = 0
        errors = []
        
        for cue in document.cues:
            if not cue.line.strip():  # ç©ºå°è¯
                skipped += 1
                continue
                
            try:
                # æå–å¤´éƒ¨å’Œå°¾éƒ¨è¯è¯­ï¼Œä¼˜å…ˆä½¿ç”¨pure_line
                text_to_process = cue.pure_line if cue.pure_line else cue.line
                head_tokens, tail_tokens = self._extract_head_tail_tokens(text_to_process)
                
                # å¯¹è¯è¯­è¿›è¡ŒG2Pè½¬æ¢
                if head_tokens:
                    head_phonemes = self.g2p_converter.batch_convert(head_tokens)
                    cue.head_tok = head_tokens
                    cue.head_phonemes = head_phonemes
                    
                if tail_tokens:
                    tail_phonemes = self.g2p_converter.batch_convert(tail_tokens)
                    cue.tail_tok = tail_tokens
                    cue.tail_phonemes = tail_phonemes
                
                processed += 1
                
            except Exception as e:
                error_msg = f"å¤„ç†å°è¯ {cue.id} å¤´å°¾è¯è¯­å¤±è´¥: {e}"
                errors.append(error_msg)
                print(f"âš ï¸ {error_msg}")
                
        print(f"âœ… å¤´å°¾è¯è¯­å¤„ç†å®Œæˆ: {processed} æ¡æˆåŠŸ, {skipped} æ¡è·³è¿‡")
        
        return {
            "processed": processed,
            "skipped": skipped,
            "errors": errors,
            "total": len(document.cues)
        }
    
    def _process_line_ngrams(self, document: SubtitleDocument, n: int = 2) -> Dict[str, Any]:
        """å¤„ç†æ•´å¥n-gramç”Ÿæˆå’ŒéŸ³ç´ è½¬æ¢"""
        if not self.g2p_converter:
            print("âš ï¸ æœªæä¾›G2Pè½¬æ¢å™¨ï¼Œè·³è¿‡æ•´å¥n-gramå¤„ç†")
            return {"processed": 0, "skipped": len(document.cues), "errors": []}
        
        processed = 0
        skipped = 0
        errors = []
        
        for cue in document.cues:
            if not cue.line.strip():  # ç©ºå°è¯
                skipped += 1
                continue
                
            try:
                # ä½¿ç”¨pure_lineç”Ÿæˆn-gramï¼Œå¦‚æœæ²¡æœ‰pure_lineåˆ™æ¸…ç†åŸline
                text_to_process = cue.pure_line if cue.pure_line else self._clean_text_for_ngram(cue.line)
                
                # ç”Ÿæˆå•è¯çº§n-gram
                tokens = self._tokenize_for_ngram(text_to_process)
                if tokens:
                    line_ngrams = self._create_ngrams(tokens, n)
                    cue.line_ngram = line_ngrams
                    
                    # å¯¹æ¯ä¸ªn-gramä¸­çš„tokenè¿›è¡ŒG2Pè½¬æ¢ï¼Œç„¶åç”ŸæˆéŸ³ç´ n-gram
                    phoneme_ngrams = []
                    for ngram in line_ngrams:
                        # å°†ngramä¸­çš„æ¯ä¸ªtokenè½¬æ¢ä¸ºéŸ³ç´ 
                        phoneme_tokens = []
                        for token in ngram:
                            phoneme = self.g2p_converter.convert(token)
                            if phoneme:
                                phoneme_tokens.append(phoneme)
                        
                        if phoneme_tokens:
                            phoneme_ngrams.append(tuple(phoneme_tokens))
                    
                    cue.line_ngram_phonemes = phoneme_ngrams
                
                processed += 1
                
            except Exception as e:
                error_msg = f"å¤„ç†å°è¯ {cue.id} æ•´å¥n-gramå¤±è´¥: {e}"
                errors.append(error_msg)
                print(f"âš ï¸ {error_msg}")
                
        print(f"âœ… æ•´å¥n-gramå¤„ç†å®Œæˆ: {processed} æ¡æˆåŠŸ, {skipped} æ¡è·³è¿‡")
        
        return {
            "processed": processed,
            "skipped": skipped,
            "errors": errors,
            "total": len(document.cues)
        }
    
    def _clean_text_for_ngram(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ç”¨äºn-gramç”Ÿæˆ"""
        import re
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆåŒ…æ‹¬"-"ï¼‰ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡å­—ç¬¦å’Œç©ºæ ¼
        cleaned = re.sub(r'[^\w\s]', ' ', text)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned
    
    def _tokenize_for_ngram(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†è¯ç”¨äºn-gramç”Ÿæˆ"""
        if not text:
            return []
        return [token.strip() for token in text.split() if token.strip()]
    
    def _create_ngrams(self, tokens: List[str], n: int = 2) -> List[tuple]:
        """åˆ›å»ºn-gramåˆ—è¡¨"""
        if len(tokens) < n:
            return []
        
        ngrams = []
        for i in range(len(tokens) - n + 1):
            ngram = tuple(tokens[i:i + n])
            ngrams.append(ngram)
        
        return ngrams
    
    def _extract_head_tail_tokens(self, line: str) -> Tuple[List[str], List[str]]:
        """
        ä»å°è¯ä¸­æå–å¤´éƒ¨å’Œå°¾éƒ¨çš„Nä¸ªè¯è¯­
        
        Args:
            line: å°è¯æ–‡æœ¬
            
        Returns:
            Tuple[List[str], List[str]]: (å¤´éƒ¨è¯è¯­åˆ—è¡¨, å°¾éƒ¨è¯è¯­åˆ—è¡¨)
        """
        # ç®€å•çš„è¯è¯­åˆ†å‰²ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦ä½¿ç”¨æ›´å¤æ‚çš„åˆ†è¯å™¨ï¼‰
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶åˆ†å‰²
        import re
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆåŒ…æ‹¬"-"ï¼‰ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡å­—ç¬¦å’Œç©ºæ ¼
        cleaned_line = re.sub(r'[^\w\s]', ' ', line)
        tokens = [token.strip() for token in cleaned_line.split() if token.strip()]
        
        if not tokens:
            return [], []
            
        # æå–å¤´éƒ¨è¯è¯­
        head_count = min(self.head_tail_count, len(tokens))
        head_tokens = tokens[:head_count]
        
        # æå–å°¾éƒ¨è¯è¯­ï¼ˆé¿å…ä¸å¤´éƒ¨é‡å¤ï¼‰
        if len(tokens) <= self.head_tail_count:
            # å¦‚æœæ€»è¯è¯­æ•°ä¸è¶…è¿‡Nï¼Œå°¾éƒ¨å°±æ˜¯å‰©ä½™çš„éƒ¨åˆ†
            tail_tokens = tokens[head_count:] if head_count < len(tokens) else []
        else:
            # å¦‚æœæ€»è¯è¯­æ•°è¶…è¿‡Nï¼Œå–æœ€åNä¸ª
            tail_tokens = tokens[-self.head_tail_count:]
            
        return head_tokens, tail_tokens
        
    def _generate_load_report(self, document: SubtitleDocument, g2p_results: Dict[str, Any], head_tail_results: Optional[Dict[str, Any]] = None, ngram_results: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ç”ŸæˆåŠ è½½æŠ¥å‘Š"""
        report = {
            "file_info": {
                "title": document.meta.title,
                "author": document.meta.author,
                "version": document.meta.version,
                "languages": document.meta.language,
                "total_cues": len(document.cues),
                "hash": document.meta.hash
            },
            "validation": self.validation_results,
            "g2p_processing": g2p_results,
            "head_tail_processing": head_tail_results or {},
            "ngram_processing": ngram_results or {},
            "summary": {
                "valid_cues": len([c for c in document.cues if c.line.strip()]),
                "empty_cues": len([c for c in document.cues if not c.line.strip()]),
                "cues_with_phonemes": len([c for c in document.cues if c.phonemes and c.phonemes.strip()]),
                "cues_with_head_tail": len([c for c in document.cues if c.head_tok or c.tail_tok]),
                "cues_with_line_ngrams": len([c for c in document.cues if c.line_ngram]),
                "characters": len(set(c.character for c in document.cues if c.character)),
                "available_languages": getattr(document, 'get_all_languages', lambda: [])()
            }
        }
        
        return report
        
    def print_load_report(self, report: Dict[str, Any]):
        """æ‰“å°åŠ è½½æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ å‰§æœ¬åŠ è½½æŠ¥å‘Š")
        print("="*60)
        
        # æ–‡ä»¶ä¿¡æ¯
        file_info = report["file_info"]
        print(f"ğŸ“ æ ‡é¢˜: {file_info['title']}")
        print(f"ğŸ‘¤ ä½œè€…: {file_info['author'] or 'æœªçŸ¥'}")
        print(f"ğŸ“… ç‰ˆæœ¬: {file_info['version']}")
        print(f"ğŸŒ è¯­è¨€: {', '.join(file_info['languages']) if file_info['languages'] else 'æœªæŒ‡å®š'}")
        if file_info.get('hash'):
            print(f"ğŸ” æ–‡ä»¶å“ˆå¸Œ: {file_info['hash'][:16]}...")
        
        # ç»Ÿè®¡ä¿¡æ¯
        summary = report["summary"]
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»å°è¯æ•°: {file_info['total_cues']}")
        print(f"   æœ‰æ•ˆå°è¯: {summary['valid_cues']}")
        print(f"   ç©ºå°è¯: {summary['empty_cues']}")
        print(f"   å·²æœ‰éŸ³ç´ : {summary['cues_with_phonemes']}")
        print(f"   å·²å¤„ç†å¤´å°¾: {summary.get('cues_with_head_tail', 0)}")
        print(f"   å·²ç”Ÿæˆn-gram: {summary.get('cues_with_line_ngrams', 0)}")
        print(f"   è§’è‰²æ•°é‡: {summary['characters']}")
        print(f"   å¯ç”¨è¯­è¨€: {', '.join(summary['available_languages']) if summary['available_languages'] else 'æ— '}")
        
        # G2På¤„ç†ç»“æœ
        g2p = report["g2p_processing"]
        if isinstance(g2p, dict) and not g2p.get("from_cache", False):
            if g2p.get("processed", 0) > 0 or g2p.get("errors"):
                print(f"\nğŸ”¤ G2På¤„ç†ç»“æœ:")
                print(f"   å·²å¤„ç†: {g2p.get('processed', 0)}")
                print(f"   å·²è·³è¿‡: {g2p.get('skipped', 0)}")
                if g2p.get("errors"):
                    print(f"   é”™è¯¯: {len(g2p['errors'])}")
                    for error in g2p["errors"][:3]:
                        print(f"     - {error}")
        
        # å¤´å°¾è¯è¯­å¤„ç†ç»“æœ
        head_tail = report.get("head_tail_processing", {})
        if isinstance(head_tail, dict) and not head_tail.get("from_cache", False):
            if head_tail.get("processed", 0) > 0 or head_tail.get("errors"):
                print(f"\nğŸ¯ å¤´å°¾è¯è¯­å¤„ç†ç»“æœ:")
                print(f"   å·²å¤„ç†: {head_tail.get('processed', 0)}")
                print(f"   å·²è·³è¿‡: {head_tail.get('skipped', 0)}")
                if head_tail.get("errors"):
                    print(f"   é”™è¯¯: {len(head_tail['errors'])}")
                    for error in head_tail["errors"][:3]:
                        print(f"     - {error}")
        
        # n-gramå¤„ç†ç»“æœ
        ngram = report.get("ngram_processing", {})
        if isinstance(ngram, dict) and not ngram.get("from_cache", False):
            if ngram.get("processed", 0) > 0 or ngram.get("errors"):
                print(f"\nğŸ”— n-gramç”Ÿæˆç»“æœ:")
                print(f"   å·²å¤„ç†: {ngram.get('processed', 0)}")
                print(f"   å·²è·³è¿‡: {ngram.get('skipped', 0)}")
                if ngram.get("errors"):
                    print(f"   é”™è¯¯: {len(ngram['errors'])}")
                    for error in ngram["errors"][:3]:
                        print(f"     - {error}")
                        
        # éªŒè¯é—®é¢˜
        validation = report["validation"]
        if isinstance(validation, dict) and not validation.get("from_cache", False):
            if validation.get("errors"):
                print(f"\nâš ï¸ éªŒè¯é—®é¢˜: {len(validation['errors'])}")
                for error in validation["errors"][:3]:
                    print(f"   - {error}")
        
        # ç¼“å­˜ä¿¡æ¯
        if any(result.get("from_cache", False) for result in [g2p, head_tail, validation]):
            print(f"\nğŸ’¾ ä»ç¼“å­˜åŠ è½½")
                
        print("="*60)


def demo_enhanced_loader():
    """æ¼”ç¤ºå¢å¼ºç‰ˆåŠ è½½å™¨"""
    print("ğŸ§ª æ¼”ç¤ºå¢å¼ºç‰ˆå‰§æœ¬åŠ è½½å™¨\n")
    
    # å‡è®¾æœ‰G2Pè½¬æ¢å™¨
    from app.core.g2p.g2p_manager import G2PManager
    
    try:
        g2p_manager = G2PManager()
        g2p_converter = g2p_manager.get_current_engine()
    except:
        g2p_converter = None
        print("âš ï¸ G2Pè½¬æ¢å™¨ä¸å¯ç”¨ï¼Œå°†è·³è¿‡éŸ³ç´ å¤„ç†")
        
    loader = EnhancedScriptLoader(g2p_converter)
    
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    test_file = "scripts/final_script.json"
    
    try:
        document, report = loader.load_script(test_file)
        loader.print_load_report(report)
        
        # æ˜¾ç¤ºå‰å‡ æ¡å°è¯
        print(f"\nğŸ“ å‰3æ¡å°è¯é¢„è§ˆ:")
        for i, cue in enumerate(document.cues[:3]):
            if cue.line.strip():
                print(f"   {cue.id}. {cue.character or '(èˆå°æç¤º)'}: {cue.line}")
                if cue.phonemes:
                    print(f"      éŸ³ç´ : {cue.phonemes[:50]}...")
                    
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")


if __name__ == "__main__":
    demo_enhanced_loader()
